{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f573389-89aa-4eef-b8e4-71471d296f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utilities import encode_one_data_set, decode_batch_predictions, num_to_char\n",
    "from test_config import TestConfiguration\n",
    "config = TestConfiguration()\n",
    "\n",
    "IMAGE_SET_NAME = 'standley_4058_test' # config.IMAGE_SET_NAME\n",
    "METADATA_FILENAME = 'words_metadata.csv' # config.METADATA_FILE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859eb83-c121-408d-9f94-1b5456918e14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the test set locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d23c9b-bf4b-44fd-bb99-6ed91ec74afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test set from Google Cloud Storage\n",
    "IMAGE_BUCKET = 'fmnh_datasets'\n",
    "\n",
    "storage_path = f'gs://{IMAGE_BUCKET}/{IMAGE_SET_NAME}/'\n",
    "!gsutil -m cp -r $storage_path ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff0c2fe3-bc30-4872-a7fc-9d6b8e38a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id    barcode  block  paragraph  word gcv_identification  \\\n",
      "0  C0047076F-b4p0w0  C0047076F      4          0     0          Asplenium   \n",
      "1  C0047076F-b4p0w1  C0047076F      4          0     1           Brakleji   \n",
      "2  C0047076F-b4p0w2  C0047076F      4          0     2               p.c.   \n",
      "3  C0047076F-b4p0w3  C0047076F      4          0     3              Eaton   \n",
      "4  C0047076F-b4p0w4  C0047076F      4          0     4             clefta   \n",
      "\n",
      "                           zooniverse_image_location  handwritten  \\\n",
      "0  file_resources\\processed_images_zooniverse-Ste...         True   \n",
      "1  file_resources\\processed_images_zooniverse-Ste...         True   \n",
      "2  file_resources\\processed_images_zooniverse-Ste...         True   \n",
      "3  file_resources\\processed_images_zooniverse-Ste...         True   \n",
      "4  file_resources\\processed_images_zooniverse-Ste...         True   \n",
      "\n",
      "  transcription  unclear  seen_count  confidence    status   collector  \\\n",
      "0     Asplenium    False           8       0.875  Complete  Steyermark   \n",
      "1      Bradleyi    False           8       0.750  Complete  Steyermark   \n",
      "2          D.C.    False           8       0.875  Complete  Steyermark   \n",
      "3         Eaton    False           8       1.000  Complete  Steyermark   \n",
      "4        clefts    False           8       0.875  Complete  Steyermark   \n",
      "\n",
      "                     full_size_image_location             image_location  \\\n",
      "0  images\\Steyermark-2021_04_14\\C0047076F.jpg  C0047076F-b4p0w0-word.jpg   \n",
      "1  images\\Steyermark-2021_04_14\\C0047076F.jpg  C0047076F-b4p0w1-word.jpg   \n",
      "2  images\\Steyermark-2021_04_14\\C0047076F.jpg  C0047076F-b4p0w2-word.jpg   \n",
      "3  images\\Steyermark-2021_04_14\\C0047076F.jpg  C0047076F-b4p0w3-word.jpg   \n",
      "4  images\\Steyermark-2021_04_14\\C0047076F.jpg  C0047076F-b4p0w4-word.jpg   \n",
      "\n",
      "        word_image_basenames  \n",
      "0  C0047076F-b4p0w0-word.jpg  \n",
      "1  C0047076F-b4p0w1-word.jpg  \n",
      "2  C0047076F-b4p0w2-word.jpg  \n",
      "3  C0047076F-b4p0w3-word.jpg  \n",
      "4  C0047076F-b4p0w4-word.jpg  \n",
      "\n",
      "Number of images in test set: 405\n",
      "\n",
      "Testing images (405) and labels (405) loaded.\n",
      "Tensor(\"args_0:0\", shape=(), dtype=string) is: <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "decode: Tensor(\"args_0:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(IMAGE_SET_NAME)\n",
    "\n",
    "metadata = pd.read_csv(Path(data_dir, METADATA_FILENAME))\n",
    "metadata['word_image_basenames'] = metadata['image_location'].map(lambda b: b.split('\\\\')[-1])\n",
    "print(metadata.head())\n",
    "\n",
    "images = list()\n",
    "images.extend(data_dir.rglob('*.png'))\n",
    "images.extend(data_dir.rglob('*.jpg'))\n",
    "images = sorted(list(map(str, images)))\n",
    "print(f'\\nNumber of images in test set: {len(images)}\\n')\n",
    "\n",
    "labels = list()\n",
    "labels = [os.path.basename(l) for l in images]\n",
    "labels = [metadata[metadata['word_image_basenames'] == b] for b in labels]\n",
    "labels = [b['transcription'].item() for b in labels]\n",
    "labels = [str(e).ljust(config.MAX_LABEL_LENGTH) for e in labels]\n",
    "\n",
    "test_images = np.array(images)\n",
    "test_labels = np.array(labels)\n",
    "print(f'Testing images ({test_images.shape[0]}) and labels ({test_labels.shape[0]}) loaded.')\n",
    "\n",
    "test_dataset = encode_one_data_set(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff72c2-b9f3-4958-add2-5af98f537d14",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f6e73-f1e3-47bb-b935-24c8207da198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from Google Cloud Storage\n",
    "MODEL_BUCKET = 'iam-model-staging'\n",
    "MODEL_NAME = 'run_55_all'\n",
    "model_uri = f'gs://{MODEL_BUCKET}/{MODEL_NAME}/model'\n",
    "!gsutil -m cp -r $model_uri .\n",
    "prediction_model_filename = Path('./model')\n",
    "prediction_model = tf.keras.models.load_model(prediction_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e65ac510-5057-4a7c-8e03-bdef5c9699ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from local filesystem\n",
    "MODEL_NAME = 'fine_tuned-prediction'\n",
    "model_location = Path(f'../transfer_learning/{MODEL_NAME}.model')\n",
    "prediction_model = tf.keras.models.load_model(model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d94d42b7-e3e2-4022-a9f3-f00335c7b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "prediction_model.compile(optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bec88a-03be-4068-a6c8-563ddc65a2ba",
   "metadata": {},
   "source": [
    "## Prediction generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f3954de-b7f9-4fc8-a44c-13e4df224852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 41ms/step\n",
      "2/2 [==============================] - 0s 85ms/step\n",
      "2/2 [==============================] - 0s 65ms/step\n",
      "2/2 [==============================] - 0s 61ms/step\n",
      "2/2 [==============================] - 0s 60ms/step\n",
      "2/2 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "           label   prediction\n",
      "0             .)            \"\n",
      "1          Maxon         Maan\n",
      "2    delitescens  dillilconan\n",
      "3       radicans     Pilicana\n",
      "4           Damp         trtt\n",
      "..           ...          ...\n",
      "400     ropinqua    pospirnou\n",
      "401    macrosora        aarsu\n",
      "402          Chr          po.\n",
      "403   Polypodium    Beleuiaem\n",
      "404       plesio          Pas\n",
      "\n",
      "[405 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "prediction_results = pd.DataFrame(columns=['label', 'prediction'])\n",
    "for batch in test_dataset:\n",
    "    images = batch['image']\n",
    "    labels = batch['label']\n",
    "    preds = prediction_model.predict(images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "    pred_texts = [t.replace('[UNK]', '').replace(' ', '') for t in pred_texts]\n",
    "    orig_texts = []\n",
    "    for label in labels:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        orig_texts.append(label)\n",
    "    orig_texts = [t.replace('[UNK]', '').replace(' ', '') for t in orig_texts]\n",
    "    new_results = pd.DataFrame(zip(orig_texts, pred_texts), columns=['label', 'prediction'])\n",
    "    prediction_results = prediction_results.append(new_results, ignore_index=True)\n",
    "print(prediction_results)\n",
    "if not os.path.exists('predictions'):\n",
    "    os.makedirs('predictions')\n",
    "prediction_results.to_csv(Path('predictions', f'{MODEL_NAME}-predictions.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
