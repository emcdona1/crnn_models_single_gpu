{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f573389-89aa-4eef-b8e4-71471d296f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utilities import encode_one_data_set, decode_batch_predictions, num_to_char\n",
    "from test_config import TestConfiguration\n",
    "config = TestConfiguration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859eb83-c121-408d-9f94-1b5456918e14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the test set locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d23c9b-bf4b-44fd-bb99-6ed91ec74afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "IMAGE_BUCKET = 'fmnh_datasets'\n",
    "MODEL_BUCKET = 'iam-model-staging'\n",
    "MODEL_NAME = 'run_55_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03b45d-caa2-428e-9e4a-d6416e735a70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download test set from Google Cloud Storage\n",
    "storage_path = f'gs://{IMAGE_BUCKET}/{config.IMAGE_SET_NAME}/'\n",
    "!gsutil -m cp -r $storage_path ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0c2fe3-bc30-4872-a7fc-9d6b8e38a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    image_location transcription error_value  \\\n",
      "0  resources\\words\\a01\\a01-000u\\a01-000u-00-00.png             A          ok   \n",
      "1  resources\\words\\a01\\a01-000u\\a01-000u-00-01.png          MOVE          ok   \n",
      "2  resources\\words\\a01\\a01-000u\\a01-000u-00-02.png            to          ok   \n",
      "3  resources\\words\\a01\\a01-000u\\a01-000u-00-03.png          stop          ok   \n",
      "4  resources\\words\\a01\\a01-000u\\a01-000u-00-04.png           Mr.          ok   \n",
      "\n",
      "  word_image_basenames  \n",
      "0   a01-000u-00-00.png  \n",
      "1   a01-000u-00-01.png  \n",
      "2   a01-000u-00-02.png  \n",
      "3   a01-000u-00-03.png  \n",
      "4   a01-000u-00-04.png  \n",
      "\n",
      "Number of images in test set: 150\n",
      "\n",
      "Testing images (150) and labels (150) loaded.\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(config.IMAGE_SET_NAME)\n",
    "\n",
    "metadata = pd.read_csv(Path(data_dir, config.METADATA_FILE_NAME))\n",
    "metadata['word_image_basenames'] = metadata['image_location'].map(lambda b: b.split('\\\\')[-1])\n",
    "print(metadata.head())\n",
    "\n",
    "images = sorted(list(map(str, list(data_dir.rglob(f'*.{config.IMAGE_FORMAT}')))))\n",
    "print(f'\\nNumber of images in test set: {len(images)}\\n')\n",
    "\n",
    "labels = list()\n",
    "labels = [os.path.basename(l) for l in images]\n",
    "labels = [metadata[metadata['word_image_basenames'] == b] for b in labels]\n",
    "labels = [b['transcription'].item() for b in labels]\n",
    "labels = [str(e).ljust(config.MAX_LABEL_LENGTH) for e in labels]\n",
    "\n",
    "test_images = np.array(images)\n",
    "test_labels = np.array(labels)\n",
    "print(f'Testing images ({test_images.shape[0]}) and labels ({test_labels.shape[0]}) loaded.')\n",
    "test_dataset = encode_one_data_set(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff72c2-b9f3-4958-add2-5af98f537d14",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "273f6e73-f1e3-47bb-b935-24c8207da198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://iam-model-staging/run_55_all/model/keras_metadata.pb...\n",
      "Copying gs://iam-model-staging/run_55_all/model/run_55_all-training_history.csv...\n",
      "Copying gs://iam-model-staging/run_55_all/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://iam-model-staging/run_55_all/model/saved_model.pb...\n",
      "Copying gs://iam-model-staging/run_55_all/model/variables/variables.index...    \n",
      "| [5/7 files][ 83.0 MiB/ 83.0 MiB]  99% Done                                    \r"
     ]
    }
   ],
   "source": [
    "model_uri = f'gs://{MODEL_BUCKET}/{MODEL_NAME}/model'\n",
    "!gsutil -m cp -r $model_uri ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d94d42b7-e3e2-4022-a9f3-f00335c7b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model_filename = Path('./model')\n",
    "prediction_model = tf.keras.models.load_model(prediction_model_filename)\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "prediction_model.compile(optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bec88a-03be-4068-a6c8-563ddc65a2ba",
   "metadata": {},
   "source": [
    "## Prediction generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f3954de-b7f9-4fc8-a44c-13e4df224852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          label  prediction\n",
      "0             A           A\n",
      "1          more        more\n",
      "2             a           a\n",
      "3          Foot        Foot\n",
      "4           and         and\n",
      "..          ...         ...\n",
      "145           .           .\n",
      "146           .           .\n",
      "147        have        have\n",
      "148        said        said\n",
      "149  Government  Government\n",
      "\n",
      "[150 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "prediction_results = pd.DataFrame(columns=['label', 'prediction'])\n",
    "for batch in test_dataset:\n",
    "    images = batch['image']\n",
    "    labels = batch['label']\n",
    "    preds = prediction_model.predict(images)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "    pred_texts = [t.replace('[UNK]', '').replace(' ', '') for t in pred_texts]\n",
    "    orig_texts = []\n",
    "    for label in labels:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        orig_texts.append(label)\n",
    "    orig_texts = [t.replace('[UNK]', '').replace(' ', '') for t in orig_texts]\n",
    "    new_results = pd.DataFrame(zip(orig_texts, pred_texts), columns=['label', 'prediction'])\n",
    "    prediction_results = prediction_results.append(new_results, ignore_index=True)\n",
    "print(prediction_results)\n",
    "prediction_results.to_csv(Path('predictions', f'{MODEL_NAME}-predictions.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2 [conda env:root] * (Local)",
   "language": "python",
   "name": "local-conda-root-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
