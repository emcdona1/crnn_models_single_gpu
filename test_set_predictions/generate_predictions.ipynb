{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f573389-89aa-4eef-b8e4-71471d296f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859eb83-c121-408d-9f94-1b5456918e14",
   "metadata": {},
   "source": [
    "## Test set loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03b45d-caa2-428e-9e4a-d6416e735a70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download test set from Google Cloud Storage\n",
    "!gsutil -m cp -r \"gs://fmnh_datasets/IAM_Words_test/\" .\n",
    "# !gsutil -m cp -r \"gs://fmnh_datasets/IAM_Words_test/\" .\n",
    "# !gsutil -m cp -r \"gs://fmnh_datasets/IAM_Words_test/\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff0c2fe3-bc30-4872-a7fc-9d6b8e38a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10831\n",
      "Testing images (10831) and labels (10831) loaded.\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path('IAM_Words_test')\n",
    "metadata_file_name = 'word_metadata2.csv'\n",
    "CHAR_LIST: str = '\\' !\"#&()[]*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "MAX_LABEL_LENGTH = 30\n",
    "IMAGE_FORMAT = 'png'\n",
    "BATCH_SIZE = 64\n",
    "characters = sorted(set(list(CHAR_LIST)))\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(data_dir, metadata_file_name))\n",
    "metadata.head()\n",
    "\n",
    "images = sorted(list(map(str, list(data_dir.rglob(f'*.{IMAGE_FORMAT}')))))\n",
    "print(len(images))\n",
    "labels = list()\n",
    "\n",
    "metadata['word_image_basenames'] = metadata['image_location'].map(lambda b: b.split('\\\\')[-1])\n",
    "labels = [os.path.basename(l) for l in images]\n",
    "labels = [metadata[metadata['word_image_basenames'] == b] for b in labels]\n",
    "labels = [b['transcription'].item() for b in labels]\n",
    "labels = [str(e).ljust(MAX_LABEL_LENGTH) for e in labels]\n",
    "\n",
    "test_images = np.array(images)\n",
    "test_labels = np.array(labels)\n",
    "\n",
    "print(f'Testing images ({test_images.shape[0]}) and labels ({test_labels.shape[0]}) loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0e3b5ed-f429-4003-9cdf-f0a58673a9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': TensorSpec(shape=(None, 400, 100, 1), dtype=tf.float32, name=None), 'label': TensorSpec(shape=(None, None), dtype=tf.int64, name=None)}\n"
     ]
    }
   ],
   "source": [
    "# Desired image dimensions\n",
    "img_width = 400\n",
    "img_height = 100\n",
    "\n",
    "# Factor by which the image is going to be downsampled\n",
    "# by the convolutional blocks. We will be using two\n",
    "# convolution blocks and each block will have\n",
    "# a pooling layer which downsample the features by a factor of 2.\n",
    "# Hence total downsampling factor would be 4.\n",
    "downsample_factor = 4\n",
    "\n",
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(characters), mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "def encode_single_sample(img_path, label):\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    # img = tf.io.decode_png(img, channels=1)\n",
    "    img = tf.io.decode_jpeg(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    # 5. Transpose the image because we want the time\n",
    "    # dimension to correspond to the width of the image.\n",
    "    img = tf.transpose(img, perm=[1, 0, 2])\n",
    "    # 6. Map the characters in label to numbers\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    return {\"image\": img, \"label\": label}\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_dataset = (\n",
    "    test_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "print(test_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff72c2-b9f3-4958-add2-5af98f537d14",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "273f6e73-f1e3-47bb-b935-24c8207da198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://iam-model-staging/run_55_all/model/keras_metadata.pb...\n",
      "Copying gs://iam-model-staging/run_55_all/model/run_55_all-training_history.csv...\n",
      "Copying gs://iam-model-staging/run_55_all/model/saved_model.pb...               \n",
      "Copying gs://iam-model-staging/run_55_all/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://iam-model-staging/run_55_all/model/variables/variables.index...\n",
      "- [5/7 files][ 83.0 MiB/ 83.0 MiB]  99% Done                                    \r"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'run_55_all'\n",
    "model_uri = f'gs://iam-model-staging/{MODEL_NAME}/model'\n",
    "!gsutil -m cp -r $model_uri ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d94d42b7-e3e2-4022-a9f3-f00335c7b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model_filename = Path('./model')\n",
    "prediction_model = tf.keras.models.load_model(prediction_model_filename)\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "prediction_model.compile(optimizer=opt)\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = tf.keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, :MAX_LABEL_LENGTH\n",
    "    ]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode('utf-8')\n",
    "        output_text.append(res)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bec88a-03be-4068-a6c8-563ddc65a2ba",
   "metadata": {},
   "source": [
    "## Prediction generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f3954de-b7f9-4fc8-a44c-13e4df224852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['label'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        label prediction\n",
      "0           A          A\n",
      "1        more       more\n",
      "2           a          a\n",
      "3        Foot       Foot\n",
      "4         and        and\n",
      "...       ...        ...\n",
      "10826      In         In\n",
      "10827  beside     beside\n",
      "10828      in         in\n",
      "10829       '          \"\n",
      "10830    went       went\n",
      "\n",
      "[10831 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "prediction_results = pd.DataFrame(columns=['label', 'prediction'])\n",
    "for batch in test_dataset:\n",
    "    images = batch['image']\n",
    "    labels = batch['label']\n",
    "    preds = prediction_model.predict(batch)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "    pred_texts = [t.replace('[UNK]', '').replace(' ', '') for t in pred_texts]\n",
    "    orig_texts = []\n",
    "    for label in labels:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        orig_texts.append(label)\n",
    "    orig_texts = [t.replace('[UNK]', '').replace(' ', '') for t in orig_texts]\n",
    "    new_results = pd.DataFrame(zip(orig_texts, pred_texts), columns=['label', 'prediction'])\n",
    "    prediction_results = prediction_results.append(new_results, ignore_index=True)\n",
    "print(prediction_results)\n",
    "prediction_results.to_csv(f'{MODEL_NAME}-predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2 [conda env:root] * (Local)",
   "language": "python",
   "name": "local-conda-root-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
