{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5628125-92bf-47a8-aa54-edb6ff561a2c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install asrtoolkit\n",
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein\n",
    "!pip install nltk\n",
    "\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from nltk.metrics import distance\n",
    "import string\n",
    "from difflib import Differ\n",
    "import asrtoolkit\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8bb8a-30ee-4aa2-9841-3c69e04da115",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load and prep statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81c943-1914-4c66-995d-251554a21e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list(Path('predictions').glob('*.csv'))\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "models = list()\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    new_results = pd.read_csv(filename)\n",
    "    model_label = Path(filename).stem  #.split('-')[0]\n",
    "    ground_truth_label = f'{model_label}-ground_truth'\n",
    "    new_results = new_results.rename(columns={'label': ground_truth_label, \n",
    "                            'prediction': model_label})\n",
    "    del new_results['Unnamed: 0']\n",
    "    models.append((ground_truth_label, model_label))\n",
    "    new_results[ground_truth_label] = new_results.apply(lambda r: str(r[ground_truth_label]), axis=1)\n",
    "    new_results[model_label] = new_results.apply(lambda r: str(r[model_label]), axis=1)\n",
    "    results = pd.concat([results, new_results], axis = 1)\n",
    "\n",
    "models = sorted(models, key=lambda b: b[1])\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3136a-f29f-4394-a398-b980e1b31477",
   "metadata": {},
   "outputs": [],
   "source": [
    "[a[1] for a in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a976692-854e-4168-a0b0-dd586595719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STATS = 6\n",
    "NUM_SUB_STATS = 5\n",
    "a = ['matchratio'] * NUM_SUB_STATS + ['CER'] * NUM_SUB_STATS + ['matchratio_insensitive'] * NUM_SUB_STATS + ['CER_insensitive'] * NUM_SUB_STATS + ['matchratio_nopunc'] * NUM_SUB_STATS + ['CER_nopunc'] * NUM_SUB_STATS\n",
    "b = ['mean', 'min', 'max', 'stdev', 'median'] * NUM_STATS\n",
    "headers = pd.MultiIndex.from_arrays([a,b], names=('stat', 'substats'))\n",
    "summary_results = pd.DataFrame(columns=headers)\n",
    "\n",
    "def apply_stat(model_label: str, query_label: str, summary_stat_label: str, stat_function):\n",
    "    results[query_label] = subset.apply(stat_function, axis=1)\n",
    "    summary_results.loc[model_label, (summary_stat_label, 'mean')] = results[query_label].mean()\n",
    "    summary_results.loc[model_label, (summary_stat_label, 'stdev')] = results[query_label].std()\n",
    "    summary_results.loc[model_label, (summary_stat_label, 'min')] = results[query_label].min()\n",
    "    summary_results.loc[model_label, (summary_stat_label, 'median')] = results[query_label].median()\n",
    "    summary_results.loc[model_label, (summary_stat_label, 'max')] = results[query_label].max()\n",
    "    subset[query_label] = results[query_label]\n",
    "    \n",
    "for ground_truth_label, model_label in models:\n",
    "    subset = results[[ground_truth_label, model_label]].dropna()\n",
    "    \n",
    "    apply_stat(model_label, f'{model_label} CER', 'CER', lambda r: asrtoolkit.cer(str(r[ground_truth_label]), str(r[model_label])))\n",
    "    apply_stat(model_label, f'{model_label} match ratio', 'matchratio', lambda r: fuzz.ratio(str(r[ground_truth_label]), str(r[model_label])))\n",
    "    results[f'{model_label}-edit_distance'] = subset.apply(lambda r: distance.edit_distance(str(r[ground_truth_label]), str(r[model_label])), axis=1)\n",
    "    \n",
    "    subset[f'{ground_truth_label}-lower'] = subset[ground_truth_label].map(str.lower)\n",
    "    subset[f'{model_label}-lower'] = subset[model_label].map(str.lower)\n",
    "    apply_stat(model_label, f'{model_label} CER insensitive', 'CER_insensitive', lambda r: asrtoolkit.cer(str(r[f'{ground_truth_label}-lower']), str(r[f'{model_label}-lower'])))\n",
    "    apply_stat(model_label, f'{model_label} match ratio insensitive', 'matchratio_insensitive', lambda r: fuzz.ratio(str(r[f'{ground_truth_label}-lower']), str(r[f'{model_label}-lower'])))\n",
    "    \n",
    "    subset[f'{ground_truth_label}-nopunc'] = subset[ground_truth_label].map(lambda s: s.translate(str.maketrans('', '', string.punctuation)))\n",
    "    subset = subset.loc[subset[f'{ground_truth_label}-nopunc'].str.len() > 0]\n",
    "    subset[f'{model_label}-nopunc'] = subset[model_label].map(lambda s: s.translate(str.maketrans('', '', string.punctuation)))\n",
    "    subset = subset.loc[subset[f'{model_label}-nopunc'].str.len() > 0]\n",
    "    apply_stat(model_label, f'{model_label} CER no punc', 'CER_nopunc', lambda r: asrtoolkit.cer(str(r[f'{ground_truth_label}-nopunc']), str(r[f'{model_label}-nopunc'])))\n",
    "    apply_stat(model_label, f'{model_label} match ratio no punc', 'matchratio_nopunc',  lambda r: fuzz.ratio(str(r[f'{ground_truth_label}-lower']), str(r[f'{model_label}-lower'])))\n",
    "    \n",
    "    exact = results.dropna().apply(lambda r: 1 if int(r[f'{model_label}-edit_distance']) == 0 else 0, axis=1).sum()\n",
    "    oboe = results.dropna().apply(lambda r: 1 if int(r[f'{model_label}-edit_distance']) == 1 else 0, axis=1).sum()\n",
    "    summary_results.loc[model_label, 'exact_matches'] = pd.to_numeric(exact, downcast='integer')\n",
    "    summary_results.loc[model_label, 'oboe_matches'] = pd.to_numeric(oboe, downcast='integer')\n",
    "    \n",
    "    err_range = stats.norm.interval(alpha=0.95, \n",
    "                                  loc=np.mean(results[f'{model_label} CER'].dropna()), \n",
    "                                  scale=stats.sem(results[f'{model_label} CER'].dropna()))\n",
    "    summary_results.loc[model_label, 'cer_95_error_min'] = err_range[0]\n",
    "    summary_results.loc[model_label, 'cer_95_error_max'] = err_range[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcd3b5-449c-4b8c-8899-3ddcdf50d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_results[[('CER', 'mean'), ('CER_insensitive', 'mean'), ('CER_nopunc', 'mean'), \n",
    "                       ('matchratio', 'mean'), ('matchratio_insensitive', 'mean'), ('matchratio_nopunc', 'mean')]])\n",
    "print(summary_results[['exact_matches', 'oboe_matches', 'cer_95_error_min', 'cer_95_error_max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e43d1-d862-48c3-9330-1757bbed3d8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate and display graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd328914-d31c-40d7-bec4-4c53ade2d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cer(graph):\n",
    "    graph.set_title('Character Error Rate (CER)')\n",
    "    graph.set_xticklabels([label[1] for label in models], rotation=15, horizontalalignment='right')\n",
    "    graph.set_ylim([-1, 200])\n",
    "    subset = results[[f'{label[1]} CER' for label in models]].T\n",
    "    subset = np.asarray([model[1].dropna() for model in subset.iterrows()], dtype=object).T\n",
    "    graph = graph.boxplot(subset)\n",
    "\n",
    "\n",
    "def plot_cer_confidence_interval(graph):\n",
    "    min = summary_results['cer_95_error_min']\n",
    "    min = min.apply(lambda v: v if v >= 0 else 0)\n",
    "    max = summary_results['cer_95_error_max']\n",
    "    max = max.apply(lambda v: v if v <= 100 else 100)\n",
    "    error_min = [summary_results.loc[label[1], ('CER', 'mean')] - min[label[1]] for label in models]\n",
    "    error_max = [max[label[1]] - summary_results.loc[label[1], ('CER', 'mean')] for label in models]\n",
    "    graph.set_title('CER 95% confidence interval')\n",
    "    graph.set_xticklabels([label[1] for label in models], rotation=15, horizontalalignment='right')\n",
    "    graph.set_ylim(top=100)\n",
    "    graph.errorbar([label[1] for label in models], summary_results[('CER', 'mean')], yerr=[error_min, error_max], fmt='o')\n",
    "    for i in range(len(models)):\n",
    "        graph.text(i + 0.05, max[i] - 2, f'{max[i]:.3f}')\n",
    "        graph.text(i + 0.05, min[i] + 1, f'{min[i]:.3f}')\n",
    "\n",
    "\n",
    "def plot_fuzzy_match(graph):\n",
    "    graph.set_title('Fuzzy Match Ratio')\n",
    "    graph.set_xticklabels([label[1] for label in models], rotation=15, horizontalalignment='right')\n",
    "    subset = results[[f'{label[1]} match ratio' for label in models]].T\n",
    "    subset = np.asarray([model[1].dropna() for model in subset.iterrows()], dtype=object).T\n",
    "    graph = graph.boxplot(subset)\n",
    "\n",
    "\n",
    "def plot_close_matches(graph):  # todo\n",
    "    graph.set_title('% of Exact/Almost Exact String Matches')\n",
    "    exact_matches = [match/results.shape[0] for match in summary_results['exact_matches']]\n",
    "    obo_matches = [match/results.shape[0] for match in summary_results['oboe_matches']]\n",
    "    ex = graph.bar([label[1] for label in models], exact_matches, \n",
    "                   width=0.3, label='Exact match')\n",
    "    obo = graph.bar([label[1] for label in models], obo_matches, \n",
    "                    width=0.3, bottom=exact_matches, label='Off by one')\n",
    "    graph.set_xticklabels([label[1] for label in models], rotation=15, horizontalalignment='right')\n",
    "    graph.set_ybound(0, 0.2)\n",
    "    graph.legend()\n",
    "\n",
    "\n",
    "fig, ((graph1, graph2, graph3, graph4)) = plt.subplots(4, 1, figsize=(15, 25))\n",
    "# fig, graph1 = plt.subplots(1, 1, figsize=(10, 7))  # For saving individual charts\n",
    "\n",
    "\n",
    "plot_cer(graph1)\n",
    "plot_cer_confidence_interval(graph2)\n",
    "plot_fuzzy_match(graph3)\n",
    "plot_close_matches(graph4)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea710b-8355-4f41-9865-d566b05cc62b",
   "metadata": {},
   "source": [
    "# OBO String Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c87c40-db0a-4343-84a5-084b4fcea5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq\n",
    "seq1 = Seq('foot')\n",
    "seq2 = Seq('moot')\n",
    "a = pairwise2.align.globalxx(seq1, seq2)\n",
    "print(pairwise2.format_alignment(*a[0]))\n",
    "a[0].end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a0007-fc46-454b-990f-72ec88f67c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Differ()\n",
    "substitution_errors = list()\n",
    "deletion_errors = [dict()]\n",
    "CHAR_LIST = '\\' !\"#&()[]*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "deletion_errors[0] = {c:0 for c in list(CHAR_LIST)}\n",
    "insertion_errors = deletion_errors * 4\n",
    "deletion_errors = deletion_errors * 4\n",
    "    \n",
    "for ground_truth_label, model_label in models:\n",
    "    oboe = results[results[f'{model_label}-edit_distance'] == 1][[ground_truth_label, model_label]]\n",
    "    subs = dict()\n",
    "    dels = deletion_errors[len(models) - 1]\n",
    "    ins = deletion_errors[len(models) - 1]\n",
    "    # print(oboe)\n",
    "    for idx, row in oboe.iterrows():\n",
    "        differ_result = list(d.compare(row[ground_truth_label], row[model_label]))\n",
    "        removed = [c[2] for c in differ_result if c[0] == '-']\n",
    "        added = [c[2] for c in differ_result if c[0] == '+']\n",
    "        assert len(removed) < 2 and len(added) < 2, f'Lens are wrong, -: {removed}, +: {added}'\n",
    "        if len(row[ground_truth_label]) == len(row[model_label]):\n",
    "            sub_chars = (removed[0], added[0])\n",
    "            # print(f'sub: {sub_chars}')\n",
    "            if sub_chars in subs.keys():\n",
    "                subs[sub_chars] += 1\n",
    "            else:\n",
    "                subs[sub_chars] = 1\n",
    "        elif len(row[ground_truth_label]) > len(row[model_label]):\n",
    "            del_char = removed[0]\n",
    "            # print(f'del: {del_char}')\n",
    "            dels[del_char] += 1\n",
    "        else:\n",
    "            ins_char = added[0]\n",
    "            # print(f'ins: {ins_char}')\n",
    "            ins[ins_char] += 1\n",
    "    substitution_errors.append(subs)\n",
    "    deletion_errors.append(dels)\n",
    "    insertion_errors.append(ins)\n",
    "substitution_errors = [dict(sorted(a.items(), key=lambda item: item[1], reverse=True)) for a in substitution_errors]\n",
    "# deletion_errors = [dict(sorted(a.items(), key=lambda item: item[1], reverse=True)) for a in deletion_errors]\n",
    "# insertion_errors = [dict(sorted(a.items(), key=lambda item: item[1], reverse=True)) for a in insertion_errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7eb8f-6833-4c73-8cb0-82f36ce5212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Top Substitutions:')\n",
    "# for key in list(substitution_errors[0])[:10]:\n",
    "#     print(f'{key}: {substitution_errors[0][key]}')\n",
    "# print('\\nTop Deletions:')\n",
    "# for key in list(deletion_errors[0])[:10]:\n",
    "#     print(f'{key}: {deletion_errors[0][key]}')\n",
    "# print('\\nTop Insertions:')\n",
    "# for key in list(insertion_errors[0])[:10]:\n",
    "#     print(f'{key}: {insertion_errors[0][key]}')\n",
    "\n",
    "plt.figure(figsize=(24,8))\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.bar(np.arange(len(deletion_errors[0].keys())) - 0.225, deletion_errors[0].values(), 0.45, label = models[0][1])\n",
    "plt.bar(np.arange(len(deletion_errors[0].keys())) + 0.225, deletion_errors[1].values(), 0.45, label = models[1][1])\n",
    "plt.xticks(np.arange(len(deletion_errors[0].keys())) , deletion_errors[0].keys())\n",
    "plt.title(\"Deletion Errors\")\n",
    "plt.ylabel(\"Number of Occurrences\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(24,8))\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.bar(np.arange(len(insertion_errors[0].keys())) - 0.225, insertion_errors[0].values(), 0.45, label = models[0][1])\n",
    "plt.bar(np.arange(len(insertion_errors[0].keys())) + 0.225, insertion_errors[1].values(), 0.45, label = models[1][1])\n",
    "plt.xticks(np.arange(len(insertion_errors[0].keys())) , insertion_errors[0].keys())\n",
    "plt.title(\"Insertion Errors\")\n",
    "plt.ylabel(\"Number of Occurrences\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f301b-981d-4c1f-a93e-dabe76c010c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_changes = list()\n",
    "for ground_truth_label, model_label in models:\n",
    "    oboes = results[results[f'{model_label}-edit_distance']==1][[f'{model_label}-ground_truth', model_label]]\n",
    "    diffs = oboes.apply(lambda r: list(d.compare(r[f'{model_label}-ground_truth'], r[model_label])), axis=1)\n",
    "    diffs = diffs.apply(lambda r: [a for a in r if a[0] == '+' or a[0] == '-'])\n",
    "    list_of_changes.append(diffs)\n",
    "print(list_of_changes[0][0:10])\n",
    "missing = list()\n",
    "added = list()\n",
    "for changes in list_of_changes:\n",
    "    missing_this_model = list()\n",
    "    added_this_model = list()\n",
    "    for change_list in changes:\n",
    "        for change in change_list:\n",
    "            if change[0] == '+':\n",
    "                added_this_model.append(change[2:])\n",
    "            elif change[0] == '-':\n",
    "                missing_this_model.append(change[2:])\n",
    "    missing.append(missing_this_model)\n",
    "    added.append(added_this_model)\n",
    "print(missing[0][0:10])\n",
    "print(added[0][0:10])\n",
    "# char_sub_summary = pd.DataFrame(columns=['Missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ca55c-844d-413e-80d5-b260d2c30285",
   "metadata": {},
   "source": [
    "# Save these results to the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb9139-3aef-4bf1-af7a-e3702ec56fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = 'results'\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "timestamp = datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n",
    "summary_results.to_csv(Path(results_folder, f'summary-{timestamp}.csv'))\n",
    "fig.savefig(Path(results_folder, f'summary-{timestamp}.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
